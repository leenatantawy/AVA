{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvAde1eHCkOZBSnYAedZfw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leenatantawy/AVA/blob/main/VisualLabs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Er7L-dGLvN_Y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor, nn\n",
        "from typing import Dict, Iterable, Optional"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_RATE = 16000\n",
        "N_FFT = 1024\n",
        "N_MELS = 128\n",
        "HOP_LENGTH = int(0.01 * SAMPLE_RATE)\n",
        "DURATION = 10\n",
        "N_SAMPLES = int(DURATION * SAMPLE_RATE)\n",
        "N_FRAMES = N_SAMPLES // HOP_LENGTH + 1"
      ],
      "metadata": {
        "id": "yxWWG9Niv3Io"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sinusoids(length, channels, max_timescale=10000):\n",
        "    \"\"\"Returns sinusoids for positional embedding\"\"\"\n",
        "    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n",
        "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n",
        "    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
        "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)"
      ],
      "metadata": {
        "id": "uJJBb3X0wQvA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MelEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    time-frequency represntation\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                sample_rate= 16000,\n",
        "                f_min=0,\n",
        "                f_max=8000,\n",
        "                n_fft=1024,\n",
        "                win_length=1024,\n",
        "                hop_length = int(0.01 * 16000),\n",
        "                n_mels = 128,\n",
        "                power = None,\n",
        "                pad= 0,\n",
        "                normalized= False,\n",
        "                center= True,\n",
        "                pad_mode= \"reflect\"\n",
        "                ):\n",
        "        super(MelEncoder, self).__init__()\n",
        "        self.window = torch.hann_window(win_length)\n",
        "        self.spec_fn = torchaudio.transforms.Spectrogram(\n",
        "            n_fft = n_fft,\n",
        "            win_length = win_length,\n",
        "            hop_length = hop_length,\n",
        "            power = power\n",
        "        )\n",
        "        self.mel_scale = torchaudio.transforms.MelScale(\n",
        "            n_mels,\n",
        "            sample_rate,\n",
        "            f_min,\n",
        "            f_max,\n",
        "            n_fft // 2 + 1)\n",
        "\n",
        "        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()\n",
        "\n",
        "    def forward(self, wav):\n",
        "        spec = self.spec_fn(wav)\n",
        "        power_spec = spec.real.abs().pow(2)\n",
        "        mel_spec = self.mel_scale(power_spec)\n",
        "        mel_spec = self.amplitude_to_db(mel_spec) # Log10(max(reference value and amin))\n",
        "        return mel_spec"
      ],
      "metadata": {
        "id": "T8KoEwvVwTEM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self, n_mels: int, n_ctx: int, audio_dim: int, text_dim: int, num_of_stride_conv: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.mel_encoder = MelEncoder(n_mels=n_mels)\n",
        "        self.conv1 = nn.Conv1d(n_mels, audio_dim, kernel_size=3, padding=1)\n",
        "        self.conv_stack = nn.ModuleList([])\n",
        "        for _ in range(num_of_stride_conv):\n",
        "            self.conv_stack.append(\n",
        "                nn.Conv1d(audio_dim, audio_dim, kernel_size=3, stride=2, padding=1)\n",
        "            )\n",
        "        # self.proj = nn.Linear(audio_dim, text_dim, bias=False)\n",
        "        self.register_buffer(\"positional_embedding\", sinusoids(n_ctx, text_dim))"
      ],
      "metadata": {
        "id": "RtsH22blwXCP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, x: Tensor):\n",
        "        \"\"\"\n",
        "        x : torch.Tensor, shape = (batch_size, waveform)\n",
        "            single channel wavform\n",
        "        \"\"\"\n",
        "        x = self.mel_encoder(x) # (batch_size, n_mels, n_ctx)\n",
        "        x = F.gelu(self.conv1(x))\n",
        "        for conv in self.conv_stack:\n",
        "            x = F.gelu(conv(x))\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = (x + self.positional_embedding).to(x.dtype)\n",
        "        return x"
      ],
      "metadata": {
        "id": "QY76XylMwbco"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCESHfONxCR7",
        "outputId": "88d45aaa-b916-4bef-8263-b7c52d7b17d2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig"
      ],
      "metadata": {
        "id": "GU6h0tsWwcbT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BartCaptionModel(nn.Module):\n",
        "    def __init__(self, n_mels=128, num_of_conv=6, sr=16000, duration=10, max_length=128, label_smoothing=0.1, bart_type=\"facebook/bart-base\", audio_dim=768):\n",
        "        super(BartCaptionModel, self).__init__()\n",
        "        # non-finetunning case\n",
        "        bart_config = BartConfig.from_pretrained(bart_type)\n",
        "        self.tokenizer = BartTokenizer.from_pretrained(bart_type)\n",
        "        self.bart = BartForConditionalGeneration(bart_config)\n",
        "\n",
        "        self.n_sample = sr * duration\n",
        "        self.hop_length = int(0.01 * sr) # hard coding hop_size\n",
        "        self.n_frames = int(self.n_sample // self.hop_length)\n",
        "        self.num_of_stride_conv = num_of_conv - 1\n",
        "        self.n_ctx = int(self.n_frames // 2**self.num_of_stride_conv) + 1\n",
        "        self.audio_encoder = AudioEncoder(\n",
        "            n_mels = n_mels, # hard coding n_mel\n",
        "            n_ctx = self.n_ctx,\n",
        "            audio_dim = audio_dim,\n",
        "            text_dim = self.bart.config.hidden_size,\n",
        "            num_of_stride_conv = self.num_of_stride_conv\n",
        "        )\n",
        "\n",
        "        self.max_length = max_length\n",
        "        self.loss_fct = nn.CrossEntropyLoss(label_smoothing= label_smoothing, ignore_index=-100)"
      ],
      "metadata": {
        "id": "vKkHvE2pxOQV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@property\n",
        "def device(self):\n",
        "    return list(self.parameters())[0].device\n"
      ],
      "metadata": {
        "id": "BmqC-R8lxPm6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shift_tokens_right(self, input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
        "        \"\"\"\n",
        "        Shift input ids one token to the right.ls\n",
        "        \"\"\"\n",
        "        shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
        "        shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
        "        shifted_input_ids[:, 0] = decoder_start_token_id\n",
        "\n",
        "        if pad_token_id is None:\n",
        "            raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
        "        # replace possible -100 values in labels by `pad_token_id`\n",
        "        shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
        "        return shifted_input_ids"
      ],
      "metadata": {
        "id": "9OnSAu8qxZsf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_encoder(self, audio):\n",
        "        audio_embs = self.audio_encoder(audio)\n",
        "        encoder_outputs = self.bart.model.encoder(\n",
        "            input_ids=None,\n",
        "            inputs_embeds=audio_embs,\n",
        "            return_dict=True\n",
        "        )[\"last_hidden_state\"]\n",
        "        return encoder_outputs, audio_embs"
      ],
      "metadata": {
        "id": "t-BwXC4Fxa4q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def forward_decoder(self, text, encoder_outputs):\n",
        "        text = self.tokenizer(text,\n",
        "                              padding='longest',\n",
        "                              truncation=True,\n",
        "                              max_length=self.max_length,\n",
        "                              return_tensors=\"pt\")\n",
        "        input_ids = text[\"input_ids\"].to(self.device)\n",
        "        attention_mask = text[\"attention_mask\"].to(self.device)\n",
        "\n",
        "        decoder_targets = input_ids.masked_fill(\n",
        "            input_ids == self.tokenizer.pad_token_id, -100\n",
        "        )\n",
        "\n",
        "        decoder_input_ids = self.shift_tokens_right(\n",
        "            decoder_targets, self.bart.config.pad_token_id, self.bart.config.decoder_start_token_id\n",
        "        )\n",
        "\n",
        "        decoder_outputs = self.bart(\n",
        "            input_ids=None,\n",
        "            attention_mask=None,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=attention_mask,\n",
        "            inputs_embeds=None,\n",
        "            labels=None,\n",
        "            encoder_outputs=(encoder_outputs,),\n",
        "            return_dict=True\n",
        "        )\n",
        "        lm_logits = decoder_outputs[\"logits\"]\n",
        "        loss = self.loss_fct(lm_logits.view(-1, self.tokenizer.vocab_size), decoder_targets.view(-1))\n",
        "        return loss"
      ],
      "metadata": {
        "id": "hKA5lZRWxh-1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, audio, text):\n",
        "        encoder_outputs, _ = self.forward_encoder(audio)\n",
        "        loss = self.forward_decoder(text, encoder_outputs)\n",
        "        return loss"
      ],
      "metadata": {
        "id": "Q4LnQoSuxlrV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(self,\n",
        "                 samples,\n",
        "                 use_nucleus_sampling=False,\n",
        "                 num_beams=5,\n",
        "                 max_length=128,\n",
        "                 min_length=2,\n",
        "                 top_p=0.9,\n",
        "                 repetition_penalty=1.0,\n",
        "                 ):\n",
        "\n",
        "        # self.bart.force_bos_token_to_be_generated = True\n",
        "        audio_embs = self.audio_encoder(samples)\n",
        "        encoder_outputs = self.bart.model.encoder(\n",
        "            input_ids=None,\n",
        "            attention_mask=None,\n",
        "            head_mask=None,\n",
        "            inputs_embeds=audio_embs,\n",
        "            output_attentions=None,\n",
        "            output_hidden_states=None,\n",
        "            return_dict=True)\n",
        "        input_ids = torch.zeros((encoder_outputs['last_hidden_state'].size(0), 1)).long().to(self.device)\n",
        "        input_ids[:, 0] = self.bart.config.decoder_start_token_id\n",
        "        decoder_attention_mask = torch.ones((encoder_outputs['last_hidden_state'].size(0), 1)).long().to(self.device)\n",
        "        if use_nucleus_sampling:\n",
        "            outputs = self.bart.generate(\n",
        "                input_ids=None,\n",
        "                attention_mask=None,\n",
        "                decoder_input_ids=input_ids,\n",
        "                decoder_attention_mask=decoder_attention_mask,\n",
        "                encoder_outputs=encoder_outputs,\n",
        "                max_length=max_length,\n",
        "                min_length=min_length,\n",
        "                do_sample=True,\n",
        "                top_p=top_p,\n",
        "                num_return_sequences=1,\n",
        "                repetition_penalty=1.1)\n",
        "        else:\n",
        "            outputs = self.bart.generate(input_ids=None,\n",
        "                                            attention_mask=None,\n",
        "                                            decoder_input_ids=input_ids,\n",
        "                                            decoder_attention_mask=decoder_attention_mask,\n",
        "                                            encoder_outputs=encoder_outputs,\n",
        "                                            head_mask=None,\n",
        "                                            decoder_head_mask=None,\n",
        "                                            inputs_embeds=None,\n",
        "                                            decoder_inputs_embeds=None,\n",
        "                                            use_cache=None,\n",
        "                                            output_attentions=None,\n",
        "                                            output_hidden_states=None,\n",
        "                                            max_length=max_length,\n",
        "                                            min_length=min_length,\n",
        "                                            num_beams=num_beams,\n",
        "                                            repetition_penalty=repetition_penalty)\n",
        "\n",
        "        captions = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        return captions"
      ],
      "metadata": {
        "id": "nNgcxtGbxpXK"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}